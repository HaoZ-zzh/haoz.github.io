---
layout: post
title: "强化学习入门"
date:   2024-9-16
tags: [RL, Reinforcement Learning]
comments: true
author: ifzzh
---

<!-- ###### 说明： -->

<!-- more -->


<link rel="stylesheet" type="text/css" href="../css/auto-title-number.css" />

## 目录

- [目录](#目录)
- [基本定义](#基本定义)
- [基本元素](#基本元素)
- [主要元素](#主要元素)
- [核心元素](#核心元素)
- [两大特点](#两大特点)
- [K-armed Bandit（多臂老虎机）](#k-armed-bandit多臂老虎机)

## 基本定义

强化学习，是在于环境的互动当中为了达成一个目标而进行的学习过程

## 基本元素

1. Agent：与环境进行互动的主体
2. Environment
3. Goal

## 主要元素

1. State：玩家和环境会处于某种state
2. Action
3. Reward：Agent在特定状态下采取了某种Action后得到的及时的反馈

## 核心元素

1. Policy：在某一个状态（输入）下，应该采取什么Action（输出）
2. Value：预期将来会得到的所有奖励之和
   1. State-Value函数 
   2. (State, Action)-Value函数：在特定状态下，采取某种行动的价值

## 两大特点

1. Trial and Error：试错学习
2. Delayed Reward：延迟奖励，行动的及时奖励可能是0，但是它对最终的胜利是有价值的（即一个行动可能没有奖励，但一定有价值）


## K-armed Bandit（多臂老虎机）

从基本元素分析：
* Agent： 我
* Environment：老虎机

假设有左右两个老虎机
状态只有一个（老虎机不会动），行动得到的奖励是及时的，做出一个选择就会得到一个奖励，不会对以后的行动有影响。只需要学习不同的行动所具有的价值，即学习`State-Action Value`函数，可以简化为`Action Value`函数。

选择一个老虎机之后，可以得到一定的奖励。那么可以认为这个奖励是一个服从一定概率分布的随机变量。两个老虎机对应的概率分布可能是不同的，但概率分布是固定的，不会改变。概率分布有一个期望值，所以最优的情况就是选期望最高的那个。

定义一个行动具有的价值为对应奖励的期望值，设选择左边的价值为`q(L)`，右边的为`q(R)`，假设左边的奖励服从`N(500,50)`，右边的服从`N(550,100)`。显然右边是最优的选择。定义行动价值的估计值为`Q(L)`和`Q(R)`，初始值为0。

利用实际获取的奖励来估计价值（如利用奖励的平均值）

用$Q_t(a)$来代表t时刻对a这个行动的价值的一个估计：

$$ Q_(a)=\frac{\sum_{i=1}^{t-1}R_i*1(A_i=a)}{\sum_{i=1}^{t-1}1(A_i=a)}, a=L,R $$

$$ A_t = argmax_aQ_t(a) (Greedy策略) $$

> 贪婪策略会导致重复选第一次选择的老虎机

一种补救的方法是，强行要求所有的行动都选择一遍，再做贪婪选择

> 由于随机性的存在，只尝试一次是不够的（我们实验的原理就是大数定律）

另一种方法是反其道而行之，因为贪婪的问题是第一次选的会成为最大值，那么我们让价值函数的初始值特别大，因此选择任何一个行动后，其对应的价值会变小，迫使我们去尝试其他行动

需要注意，价值的初始值为0时，这个初始值不计入之后的平均计算，任何行动执行一次之后，就用实际获得的奖励值替代了初始值；当我们选择一个很高的初始值，同样不计入，实际的结果就相当于把所有的行动都尝试一遍。

我们这里说的方法，是将初始值计入之后平均值的计算，如果不计入，相当于只探索一遍，只有计入才能鼓励更多的探索。

这个方法只能考虑一开始的探索行为，一旦我们考虑状态会发生改变的情况，这个方法就不可行了。

归根结底，不能太贪婪！

一种有效解决方法，成为$\epsilon-Greedy$，其含义是在大多数情况下，作出贪婪的选择；以一定的概率$\epsilon$，做出随机的选择。 
